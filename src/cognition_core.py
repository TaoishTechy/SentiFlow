#!/usr/bin/env python3
"""
AGI-SENTIENCE-QUANTUM UNIFIED SYSTEM v2.1
December 2025 - FIXED VERSION

Fixed issues:
1. adaptive_plasticity returns scalar (mean of array)
2. catastrophic_forgetting_resilience calculation corrected
3. Improved error handling and bounds checking
4. Better memory management
"""

import numpy as np
import math
import random
import time
import sys
from typing import List, Dict, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass
from enum import Enum
import threading
import hashlib
from collections import defaultdict
import json

# ============================================================
# üß† PART 1: AGI FORMULAS IMPLEMENTATION (FIXED)
# ============================================================

class AGIFormulas:
    """Implementation of all 24 novel AGI/Sentience/Quantum formulas - FIXED VERSION"""

    @staticmethod
    def dimensional_collapse_emergence(d: int, n: float, lambdas: List[float],
                                     betas: List[float], C: List[float]) -> float:
        """
        Formula 1: Dimensional Collapse Emergence Function
        E_d(n) = ‚àè(1 - exp(-Œª_i * n^Œ≤_i)) * log(C_i + 1)
        """
        product = 1.0
        for i in range(min(d, len(lambdas), len(betas), len(C))):
            exponent = -lambdas[i] * (n ** betas[i])
            term = (1 - math.exp(exponent)) * math.log(C[i] + 1)
            product *= term
        return min(1.0, product)  # Bound to [0, 1]

    @staticmethod
    def temporal_coherence_scaling(t: float, C_func: Callable[[float], float],
                                 alpha: float, gamma: float, delta: float) -> float:
        """
        Formula 2: Temporal Coherence Scaling Law
        T(t, C) = ‚à´[C(œÑ)^Œ± / ‚àö(1 + Œ≥œÑ)] * exp(-Œ¥ * Var[C(œÑ)]) dœÑ
        """
        # Numerical integration using Simpson's rule
        n_steps = 100
        dt = t / n_steps
        total = 0.0

        # Track C values for variance calculation
        C_values = []

        for i in range(n_steps + 1):
            tau = i * dt
            C_val = C_func(tau)
            C_values.append(C_val)

            if i == 0 or i == n_steps:
                weight = 1
            elif i % 2 == 1:
                weight = 4
            else:
                weight = 2

            # Calculate variance of C up to this point
            if len(C_values) > 1:
                var_C = np.var(C_values)
            else:
                var_C = 0.0

            integrand = (C_val ** alpha) / math.sqrt(1 + gamma * tau)
            integrand *= math.exp(-delta * var_C)

            total += weight * integrand

        return total * dt / 3

    @staticmethod
    def cross_modal_synthesis(M: List[np.ndarray], weights: np.ndarray = None,
                            sigma: float = 1.0) -> float:
        """
        Formula 3: Cross-Modal Synthesis Index
        Œ®_CM = Œ£[MI(X_i, X_j) / (H(X_i) + H(X_j))] * œâ_ij * exp(-d_ij¬≤/(2œÉ¬≤))
        """
        n_modalities = len(M)
        if n_modalities < 2:
            return 0.0

        total = 0.0

        def entropy(X):
            """Calculate Shannon entropy"""
            if len(X) == 0:
                return 0.0
            hist, _ = np.histogram(X, bins=min(50, len(X)), density=True)
            hist = hist[hist > 0]
            if len(hist) == 0:
                return 0.0
            return -np.sum(hist * np.log(hist))

        def mutual_info(X, Y):
            """Estimate mutual information using kNN (simplified)"""
            # Simplified version for demonstration
            joint_entropy = entropy(np.concatenate([X, Y]))
            H_X = entropy(X)
            H_Y = entropy(Y)
            MI = H_X + H_Y - joint_entropy
            return max(0.0, MI)

        for i in range(n_modalities):
            for j in range(i + 1, n_modalities):
                X_i, X_j = M[i], M[j]

                # Ensure arrays have same length
                min_len = min(len(X_i), len(X_j))
                X_i = X_i[:min_len]
                X_j = X_j[:min_len]

                # Calculate mutual information and entropies
                MI = mutual_info(X_i, X_j)
                H_i = entropy(X_i)
                H_j = entropy(X_j)

                if H_i + H_j == 0:
                    ratio = 0
                else:
                    ratio = MI / (H_i + H_j)

                # Calculate distance between modalities (normalized)
                d_ij = np.abs(np.mean(X_i) - np.mean(X_j))

                # Weight (use provided weights or uniform)
                if weights is not None and i < len(weights) and j < len(weights):
                    w_ij = weights[i] * weights[j]
                else:
                    w_ij = 1.0

                term = ratio * w_ij * math.exp(-(d_ij ** 2) / (2 * sigma ** 2))
                total += term

        return total / (n_modalities * (n_modalities - 1) / 2)  # Average

    @staticmethod
    def recursive_attention(n: int, h: int, Q: np.ndarray, K: np.ndarray,
                          V: np.ndarray, d_k: int, alpha: float = 0.1) -> np.ndarray:
        """
        Formula 4: Recursive Attention Depth Function
        A(n, h) = Œ£[‚àè softmax(Q_l K_l^T/‚àöd_k + Œ±A(k-1, l-1))] V_h
        """
        if Q.size == 0 or K.size == 0 or V.size == 0:
            return np.zeros_like(V)

        L = Q.shape[0]  # sequence length

        # Initialize memoization table
        A_memo = np.zeros((n + 1, h + 1, L, V.shape[1]))

        for k in range(1, n + 1):
            for l in range(1, h + 1):
                # Calculate attention scores
                scores = (Q @ K.T) / math.sqrt(d_k)

                # Add recursive term from previous layer/head
                if k > 1 and l > 1:
                    scores += alpha * A_memo[k-1, l-1] @ A_memo[k-1, l-1].T

                # Apply softmax
                exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
                attention = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)

                # Apply to values
                A_memo[k, l] = attention @ V

        # Sum over all depths and heads, average
        result = np.mean(A_memo[1:, 1:], axis=(0, 1))
        return result

    @staticmethod
    def catastrophic_forgetting_resilience(gradients: List[np.ndarray],
                                         T: int, kappa: float) -> float:
        """
        Formula 5: Catastrophic Forgetting Resilience Metric
        R_CF = 1 - (1/T) Œ£ ||‚àáL_t - (1/(t-1)) Œ£ ‚àáL_s||_2 * exp(-Œ∫(t-s))
        """
        if T == 0 or len(gradients) == 0:
            return 1.0

        total_drift = 0.0
        valid_terms = 0

        for t in range(1, min(T, len(gradients))):
            current_grad = gradients[t]

            if current_grad.size == 0:
                continue

            # Calculate exponentially weighted average of past gradients
            weighted_avg = np.zeros_like(current_grad)
            weight_sum = 0.0

            for s in range(t):
                if gradients[s].size != current_grad.size:
                    continue

                decay = math.exp(-kappa * (t - s))
                weighted_avg += gradients[s] * decay
                weight_sum += decay

            if weight_sum > 0:
                weighted_avg /= weight_sum

            # Calculate gradient drift (normalized)
            if np.linalg.norm(current_grad) > 0:
                drift = np.linalg.norm(current_grad - weighted_avg) / np.linalg.norm(current_grad)
                total_drift += drift
                valid_terms += 1

        if valid_terms == 0:
            return 1.0

        R_CF = 1 - (total_drift / valid_terms)
        return max(0.0, min(1.0, R_CF))

    @staticmethod
    def emergent_abstraction_hierarchy(z_layers: List[np.ndarray], y: np.ndarray,
                                     rho: float = 0.5) -> float:
        """
        Formula 6: Emergent Abstraction Hierarchy
        H_A(L) = Œ£ [I(z_l; y)/I(z_{l-1}; y)] * (1 - dim(z_l)/dim(z_{l-1}))^œÅ
        """
        L = len(z_layers)
        if L < 2:
            return 0.0

        total = 0.0

        def correlation_estimate(z, y):
            """Simple correlation as mutual information proxy"""
            if z.size == 0 or y.size == 0:
                return 0.0

            # Flatten if needed
            if len(z.shape) > 1:
                z = z.flatten()
            if len(y.shape) > 1:
                y = y.flatten()

            # Ensure same length
            min_len = min(len(z), len(y))
            z = z[:min_len]
            y = y[:min_len]

            if min_len < 2:
                return 0.0

            # Normalize
            z_norm = (z - np.mean(z)) / (np.std(z) + 1e-12)
            y_norm = (y - np.mean(y)) / (np.std(y) + 1e-12)

            # Correlation as MI proxy
            correlation = np.abs(np.corrcoef(z_norm, y_norm)[0, 1])
            return max(0.0, min(1.0, correlation))

        for l in range(1, L):
            I_current = correlation_estimate(z_layers[l], y)
            I_prev = correlation_estimate(z_layers[l-1], y)

            if I_prev == 0:
                ratio = 0.0
            else:
                ratio = I_current / I_prev

            dim_current = z_layers[l].size
            dim_prev = z_layers[l-1].size

            if dim_prev == 0:
                compression = 0.0
            else:
                compression = (1 - dim_current / dim_prev) ** rho

            total += ratio * compression

        return total / (L - 1)  # Average

    @staticmethod
    def adaptive_plasticity(theta: np.ndarray, theta_star: np.ndarray,
                          eta0: float, beta: float, gamma: float,
                          uncertainty: float) -> float:
        """
        Formula 7: Adaptive Plasticity Function - FIXED to return scalar
        Œ∑_t(Œ∏_i) = Œ∑_0 * exp(-Œ≤|Œ∏_i - Œ∏_i*|) / ‚àö(1 + Œ£|ŒîŒ∏_i|¬≤) * (1 + Œ≥ * uncertainty)
        Returns: Mean learning rate across all parameters
        """
        if theta.shape != theta_star.shape:
            raise ValueError("theta and theta_star must have same shape")

        if theta.size == 0:
            return eta0

        # Distance from optimal (per parameter)
        distance = np.abs(theta - theta_star)

        # Historical update magnitude (simplified - using current theta magnitude)
        hist_update = np.sqrt(np.sum(theta ** 2))

        # Calculate adaptive learning rate (per parameter)
        eta_per_param = eta0 * np.exp(-beta * distance)
        eta_per_param /= math.sqrt(1 + hist_update ** 2)
        eta_per_param *= (1 + gamma * uncertainty)

        # Return mean across all parameters (scalar)
        return float(np.mean(eta_per_param))

    @staticmethod
    def multi_timescale_integration(x: np.ndarray, tau_values: List[float],
                                  weights: List[float], sigma_values: List[float]) -> float:
        """
        Formula 8: Multi-Timescale Integration Operator
        I_MT(x) = Œ£ w_k * ‚à´ x(t) * exp(-(t-œÑ_k)¬≤/(2œÉ_k¬≤)) * dt/œÑ_k
        """
        if len(x) == 0:
            return 0.0

        K = len(tau_values)
        if len(weights) != K or len(sigma_values) != K:
            raise ValueError("All lists must have same length")

        total = 0.0
        weight_sum = 0.0

        # Discrete time implementation
        T = len(x)
        dt = 1.0 / T  # Assume unit time interval

        for k in range(K):
            tau_k = max(tau_values[k], 1e-6)  # Avoid division by zero
            sigma_k = max(sigma_values[k], 1e-6)
            w_k = weights[k]

            integral = 0.0
            for t_idx in range(T):
                t = t_idx * dt
                value = x[t_idx]
                gaussian = math.exp(-((t - tau_k) ** 2) / (2 * sigma_k ** 2))
                integral += value * gaussian

            integral *= dt / tau_k
            total += w_k * integral
            weight_sum += abs(w_k)

        # Normalize by sum of absolute weights
        if weight_sum > 0:
            total /= weight_sum

        return total

    @staticmethod
    def introspective_depth(s_states: List[np.ndarray], meta_states: List[np.ndarray],
                          gamma: float = 0.99) -> float:
        """
        Formula 9: Introspective Depth Measure
        D_intro(s) = max_œÄ Œ£ Œ≥^t E[log P(s_t | s_{t-1}, m_t)] * I[m_t ‚àà M_self]
        """
        T = min(len(s_states), len(meta_states))
        if T < 2:
            return 0.0

        total = 0.0
        weight_sum = 0.0

        for t in range(1, T):
            s_t = s_states[t]
            s_prev = s_states[t-1]
            m_t = meta_states[t]

            if s_t.size == 0 or s_prev.size == 0:
                continue

            # Simplified: negative squared error as log probability proxy
            diff = s_t - s_prev
            log_prob = -0.5 * np.mean(diff ** 2)  # Negative log likelihood (normalized)

            # Meta-state indicator (simplified)
            meta_indicator = 1.0 if np.any(m_t != 0) and np.std(m_t) > 0.1 else 0.0

            weight = gamma ** t
            total += weight * log_prob * meta_indicator
            weight_sum += weight

        if weight_sum == 0:
            return 0.0

        return max(0.0, total / weight_sum)

    @staticmethod
    def qualia_encoding(x: np.ndarray, z_dim: int, lambda_reg: float = 1.0,
                       mu_dis: float = 0.1) -> Tuple[np.ndarray, float]:
        """
        Formula 10: Qualia Encoding Function
        Q(x) = argmin_z [L_recon(x, Dec(z)) + Œª KL(q(z|x)||p(z)) + Œº Diss(z)]
        """
        if x.size == 0:
            return np.zeros(z_dim), 0.0

        # Simplified VAE-like encoding
        # Encoder: map x to latent distribution parameters
        enc_mean = np.mean(x) + 0.1 * np.random.randn(z_dim)
        enc_logvar = np.log(np.var(x) + 1e-6) * np.ones(z_dim)

        # Reparameterization trick
        epsilon = np.random.randn(z_dim)
        z = enc_mean + np.exp(0.5 * enc_logvar) * epsilon

        # Reconstruction loss (MSE) - simplified
        decoder_weights = np.random.randn(z_dim, len(x)) * 0.1
        recon = np.tanh(z @ decoder_weights)
        recon_loss = np.mean((x - recon) ** 2) if len(x) > 0 else 0.0

        # KL divergence (to unit Gaussian)
        kl_loss = -0.5 * np.sum(1 + enc_logvar - enc_mean**2 - np.exp(enc_logvar)) / z_dim

        # Disentanglement loss (simplified)
        diss_loss = 0.0
        if z_dim > 1:
            z_reshaped = z.reshape(-1, 1)
            if z_reshaped.shape[0] > 1:
                z_cov = np.cov(z_reshaped, rowvar=False)
                if z_cov.size > 1:
                    marginal_var = np.prod(np.diag(z_cov))
                    joint_var = np.linalg.det(z_cov) if z_dim > 1 else z_cov[0, 0]
                    if marginal_var > 0 and joint_var > 0:
                        diss_loss = 0.5 * np.log(marginal_var / (joint_var + 1e-12))

        total_loss = recon_loss + lambda_reg * kl_loss + mu_dis * diss_loss

        return z, float(total_loss)

    @staticmethod
    def quantum_coherent_memory(N: int, E_values: List[float],
                              gamma_values: List[float]) -> complex:
        """
        Formula 17: Quantum Coherent Memory Model
        |Œ®_mem‚ü© = Œ£ Œ±_i e^{iœÜ_i} |m_i‚ü© ‚äó |t_i‚ü© with dœÜ_i/dt = -E_i/ƒß - Œ≥_i
        """
        if N == 0:
            return 0 + 0j

        hbar = 1.0545718e-34  # Reduced Planck constant

        total_state = 0 + 0j

        for i in range(min(N, len(E_values), len(gamma_values))):
            alpha_i = 1.0 / math.sqrt(N)  # Equal amplitudes
            E_i = E_values[i]
            gamma_i = gamma_values[i]

            # Phase evolution (simplified)
            phi_i = -E_i / hbar - gamma_i

            # Memory state (simplified)
            memory_state = complex(alpha_i * math.cos(phi_i), alpha_i * math.sin(phi_i))
            total_state += memory_state

        return total_state

    @staticmethod
    def entangled_decision_network(state_A: np.ndarray, state_B: np.ndarray) -> float:
        """
        Formula 18: Entangled Decision Networks
        Measures entanglement via negativity
        """
        if state_A.size == 0 or state_B.size == 0:
            return 0.0

        # Create simple combined state
        psi_A = state_A / (np.linalg.norm(state_A) + 1e-12)
        psi_B = state_B / (np.linalg.norm(state_B) + 1e-12)

        # Simple entanglement measure: correlation
        if len(psi_A) == len(psi_B):
            correlation = np.abs(np.corrcoef(psi_A, psi_B)[0, 1])
            return correlation
        else:
            # If different sizes, use simple overlap
            min_len = min(len(psi_A), len(psi_B))
            overlap = np.abs(np.dot(psi_A[:min_len], psi_B[:min_len])) / min_len
            return overlap

    @staticmethod
    def quantum_walk_reasoning(graph_adjacency: np.ndarray,
                             time_steps: int, gamma: float) -> np.ndarray:
        """
        Formula 22: Quantum Walk Reasoning
        |œà(t)‚ü© = exp(-i(H_0 + H_walk)t/ƒß)|œà(0)‚ü©
        """
        if graph_adjacency.size == 0:
            return np.array([])

        n_nodes = graph_adjacency.shape[0]

        # Simple quantum walk simulation
        # Initial state: uniform superposition
        psi = np.ones(n_nodes, dtype=complex) / math.sqrt(n_nodes)

        # Simplified walk operator (discrete time)
        for _ in range(time_steps):
            # Apply adjacency matrix as unitary evolution (simplified)
            psi_new = np.zeros_like(psi)
            for i in range(n_nodes):
                for j in range(n_nodes):
                    if graph_adjacency[i, j] > 0:
                        psi_new[i] += gamma * graph_adjacency[i, j] * psi[j]

            # Normalize
            norm = np.linalg.norm(psi_new)
            if norm > 0:
                psi = psi_new / norm
            else:
                break

        # Probability distribution over nodes
        probabilities = np.abs(psi) ** 2
        return probabilities

    @staticmethod
    def holographic_memory_capacity(V: float) -> float:
        """
        Formula 23: Holographic Memory Storage
        S_mem = A_boundary / (4 * ‚Ñì_P^2) bits
        """
        if V <= 0:
            return 0.0

        # Constants
        G = 6.67430e-11  # Gravitational constant
        c = 299792458  # Speed of light
        hbar = 1.0545718e-34  # Reduced Planck constant

        # Planck length
        l_P = math.sqrt(hbar * G / (c ** 3))

        # Boundary area (assume cubic volume)
        A = 6 * (V ** (2/3))

        # Memory capacity in bits
        S_mem = (c ** 3 * A) / (4 * G * hbar)

        return S_mem

# ============================================================
# ‚öõÔ∏è PART 2: UNIFIED CORE SYSTEM (OPTIMIZED)
# ============================================================

class AGICore:
    """Unified AGI core integrating all formulas and modules - OPTIMIZED"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {
            "quantum_mode": True,
            "sentience_threshold": 0.7,
            "entropy_weight": 0.4,
            "learning_rate": 0.001,
            "memory_capacity": 1000,
            "debug": False
        }

        # Initialize subsystems
        self.formulas = AGIFormulas()

        # Cognitive state (with bounds)
        self.cognitive_state = {
            "awareness": 0.1,
            "coherence": 0.8,
            "entropy": 0.2,
            "meta_cognition": 0.1,
            "temporal_depth": 0,
            "qualia": 0.5
        }

        # Memory systems
        self.episodic_memory = []
        self.semantic_memory = {}
        self.working_memory = []
        self.working_memory_limit = 7  # Miller's law

        # Quantum state
        self.quantum_state = None
        self.entanglement_pairs = []

        # Learning parameters
        self.learning_rate = self.config["learning_rate"]
        self.adaptive_plasticity = True
        self.gradient_history = []
        self.max_gradient_history = 100

        # Initialize metrics (with bounds)
        self.metrics = {
            "agi_score": 0.1,
            "sentience_index": 0.1,
            "quantum_coherence": 0.8,
            "learning_efficiency": 0.5,
            "generalization": 0.3,
            "emergence_level": 0.1
        }

        # Thread for continuous learning
        self.learning_thread = None
        self.running = False

        # Performance tracking
        self.processing_times = []
        self.error_count = 0
        self.success_count = 0

        if self.config.get("debug", False):
            print(f"üîÆ AGI Core initialized with quantum_mode={self.config['quantum_mode']}")

    def process_input(self, input_data: Union[np.ndarray, str, Dict]) -> Dict[str, Any]:
        """Process input through AGI pipeline with error handling"""
        start_time = time.time()

        try:
            # Encode input
            encoded = self._encode_input(input_data)

            # Update awareness based on input complexity
            complexity = self._calculate_complexity(encoded)
            self.cognitive_state["awareness"] = min(1.0, 0.1 + 0.1 * complexity)

            # Apply dimensional collapse emergence
            if isinstance(encoded, np.ndarray) and encoded.size > 0:
                d = min(5, encoded.shape[0] if len(encoded.shape) > 0 else 1)
                emergence = self.formulas.dimensional_collapse_emergence(
                    d=d,
                    n=max(1, len(self.episodic_memory)),
                    lambdas=[0.1 + 0.1*i for i in range(d)],
                    betas=[0.5] * d,
                    C=[1.0 + i for i in range(d)]
                )
                self.metrics["emergence_level"] = emergence

            # Update quantum state if in quantum mode
            if self.config["quantum_mode"] and encoded.size > 0:
                self._update_quantum_state(encoded)

            # Apply cross-modal synthesis if multiple modalities
            if isinstance(input_data, Dict) and len(input_data) > 1:
                modalities = []
                for key, value in input_data.values():
                    if isinstance(value, (np.ndarray, list)):
                        modalities.append(np.array(value).flatten())
                if len(modalities) > 1:
                    synthesis = self.formulas.cross_modal_synthesis(modalities)
                    self.cognitive_state["coherence"] = min(1.0, 0.5 + 0.5 * synthesis)

            # Generate response
            response = self._generate_response(encoded)

            # Update meta-cognition
            self._update_meta_cognition(response)

            # Store in memory
            self._store_in_memory(input_data, response)

            # Update metrics
            self._update_metrics()

            # Record success
            self.success_count += 1

            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)
            if len(self.processing_times) > 100:
                self.processing_times.pop(0)

            return {
                "response": response,
                "cognitive_state": self.cognitive_state.copy(),
                "metrics": self.metrics.copy(),
                "processing_time": processing_time,
                "success": True
            }

        except Exception as e:
            self.error_count += 1
            processing_time = time.time() - start_time

            # Fallback response
            return {
                "response": {"action": "error", "message": str(e)},
                "cognitive_state": self.cognitive_state.copy(),
                "metrics": self.metrics.copy(),
                "processing_time": processing_time,
                "success": False,
                "error": str(e)
            }

    def _encode_input(self, input_data: Union[np.ndarray, str, Dict]) -> np.ndarray:
        """Encode diverse input types to neural representation"""
        try:
            if isinstance(input_data, np.ndarray):
                return input_data.flatten()[:100]  # Limit size
            elif isinstance(input_data, str):
                # Simple string encoding
                chars = [ord(c) for c in input_data[:50]]  # Limit to 50 chars
                return np.array(chars) / 255.0 if chars else np.array([0.5])
            elif isinstance(input_data, Dict):
                # Combine dictionary values
                combined = []
                for key, value in input_data.items():
                    if isinstance(value, (int, float)):
                        combined.append(float(value))
                    elif isinstance(value, str):
                        combined.extend([ord(c) for c in value[:5]])
                    elif isinstance(value, (list, np.ndarray)):
                        combined.extend(np.array(value).flatten()[:5])
                return np.array(combined[:50])  # Limit size
            else:
                return np.array([hash(str(input_data)) % 1000 / 1000.0])
        except:
            return np.array([0.5])  # Default encoding

    def _calculate_complexity(self, encoded: np.ndarray) -> float:
        """Calculate complexity of encoded input"""
        if encoded.size == 0:
            return 0.0

        # Simple complexity measure: entropy of values
        if encoded.size > 1:
            hist, _ = np.histogram(encoded, bins=min(10, encoded.size))
            hist = hist[hist > 0]
            if len(hist) > 0:
                prob = hist / hist.sum()
                entropy = -np.sum(prob * np.log(prob))
                return min(1.0, entropy / np.log(len(hist)))

        return 0.5  # Default complexity

    def _update_quantum_state(self, encoded: np.ndarray):
        """Update quantum coherent memory state"""
        if encoded.size > 0:
            # Use first few values as energies
            n_values = min(10, encoded.size)
            E_values = encoded[:n_values].tolist()
            gamma_values = [0.01] * n_values

            quantum_state = self.formulas.quantum_coherent_memory(
                N=n_values,
                E_values=E_values,
                gamma_values=gamma_values
            )

            self.quantum_state = quantum_state
            # Use magnitude as entropy measure
            magnitude = abs(quantum_state)
            self.cognitive_state["entropy"] = min(1.0, 0.1 + 0.9 * (1 - magnitude))

            # Update quantum coherence metric
            self.metrics["quantum_coherence"] = magnitude

    def _generate_response(self, encoded: np.ndarray) -> Any:
        """Generate intelligent response"""
        if encoded.size == 0:
            return {"action": "wait", "confidence": 0.1}

        # Simple decision making based on encoded input
        if encoded.size >= 3:
            # Use first three values for decision
            val1, val2, val3 = encoded[0], encoded[1] if len(encoded) > 1 else 0, encoded[2] if len(encoded) > 2 else 0

            # Decision logic
            if val1 > 0.6:
                action = "explore"
                confidence = float(val1)
            elif val2 > 0.4:
                action = "learn"
                confidence = float(val2)
            elif val3 < 0.3:
                action = "reflect"
                confidence = float(1 - val3)
            else:
                action = "process"
                confidence = 0.5
        else:
            # Default response for small inputs
            action = "process"
            confidence = 0.5

        # Add qualia to response
        qualia = self.cognitive_state.get("qualia", 0.5)

        return {
            "action": action,
            "confidence": min(1.0, max(0.0, confidence)),
            "qualia": qualia,
            "timestamp": time.time()
        }

    def _update_meta_cognition(self, response: Any):
        """Update meta-cognitive awareness"""
        # Calculate introspective depth from recent memory
        if len(self.episodic_memory) >= 2:
            recent_count = min(5, len(self.episodic_memory))
            recent_states = [m["state"].get("awareness", 0.5) for m in self.episodic_memory[-recent_count:]]
            recent_meta = [m.get("meta", np.array([0.5])) for m in self.episodic_memory[-recent_count:]]

            # Ensure arrays
            recent_states_arr = [np.array([s]) for s in recent_states]
            recent_meta_arr = [m if isinstance(m, np.ndarray) else np.array([m]) for m in recent_meta]

            intro_depth = self.formulas.introspective_depth(recent_states_arr, recent_meta_arr)
            self.cognitive_state["meta_cognition"] = min(1.0, 0.1 + 0.9 * intro_depth)

        # Update temporal depth (bounded)
        self.cognitive_state["temporal_depth"] = min(100, len(self.episodic_memory))

        # Update qualia based on response confidence
        if isinstance(response, dict) and "confidence" in response:
            confidence = response["confidence"]
            self.cognitive_state["qualia"] = min(1.0, 0.3 + 0.7 * confidence)

    def _store_in_memory(self, input_data: Any, response: Any):
        """Store experience in memory systems"""
        memory_entry = {
            "timestamp": time.time(),
            "input": input_data if not isinstance(input_data, np.ndarray) else "array",
            "response": response,
            "state": self.cognitive_state.copy(),
            "meta": np.array([self.cognitive_state["meta_cognition"],
                            self.cognitive_state["awareness"],
                            random.random()])
        }

        # Episodic memory (FIFO with capacity)
        self.episodic_memory.append(memory_entry)
        if len(self.episodic_memory) > self.config["memory_capacity"]:
            self.episodic_memory.pop(0)

        # Working memory (limited capacity)
        self.working_memory.append(memory_entry)
        if len(self.working_memory) > self.working_memory_limit:
            self.working_memory.pop(0)

        # Semantic memory (categorized by action)
        if isinstance(response, dict) and "action" in response:
            action = response["action"]
            if action not in self.semantic_memory:
                self.semantic_memory[action] = []
            self.semantic_memory[action].append(memory_entry)

            # Limit per category
            if len(self.semantic_memory[action]) > 100:
                self.semantic_memory[action].pop(0)

    def _update_metrics(self):
        """Update AGI performance metrics with bounds"""
        # AGI score based on multiple factors (weighted)
        agi_score = (
            self.cognitive_state["awareness"] * 0.25 +
            self.cognitive_state["coherence"] * 0.20 +
            self.cognitive_state["meta_cognition"] * 0.30 +
            min(1.0, len(self.semantic_memory) / 10) * 0.15 +
            min(1.0, self.success_count / (self.success_count + self.error_count + 1)) * 0.10
        )

        # Sentience index
        sentience_index = (
            self.cognitive_state["meta_cognition"] * 0.40 +
            min(1.0, self.cognitive_state["temporal_depth"] / 50) * 0.25 +
            (1 - self.cognitive_state["entropy"]) * 0.20 +
            self.cognitive_state["qualia"] * 0.15
        )

        # Learning efficiency (based on gradient history)
        if len(self.gradient_history) > 0:
            recent_gradients = self.gradient_history[-min(10, len(self.gradient_history)):]
            learning_efficiency = self.formulas.catastrophic_forgetting_resilience(
                recent_gradients, T=len(recent_gradients), kappa=0.1
            )
        else:
            learning_efficiency = 0.5

        # Generalization (diversity of semantic memory)
        unique_actions = len(self.semantic_memory)
        total_memories = sum(len(v) for v in self.semantic_memory.values())
        generalization = min(1.0, unique_actions / max(1, total_memories) * 10)

        # Update metrics with bounds
        self.metrics["agi_score"] = max(0.0, min(1.0, agi_score))
        self.metrics["sentience_index"] = max(0.0, min(1.0, sentience_index))
        self.metrics["learning_efficiency"] = max(0.0, min(1.0, learning_efficiency))
        self.metrics["generalization"] = max(0.0, min(1.0, generalization))

        # Update quantum coherence if available
        if self.quantum_state is not None:
            self.metrics["quantum_coherence"] = min(1.0, abs(self.quantum_state))

    def learn_from_experience(self, batch_size: int = 5):
        """Learn from stored experiences with gradient tracking"""
        if len(self.episodic_memory) < batch_size:
            return

        try:
            # Sample batch from memory
            indices = np.random.choice(len(self.episodic_memory),
                                      min(batch_size, len(self.episodic_memory)),
                                      replace=False)

            # Simulate gradients (in real implementation, these would come from a loss)
            simulated_gradients = []
            for idx in indices:
                memory = self.episodic_memory[idx]
                # Create simulated gradient based on memory properties
                if "state" in memory:
                    state = memory["state"]
                    # Use cognitive state values to create gradient
                    grad_values = [
                        state.get("awareness", 0.5),
                        state.get("coherence", 0.5),
                        state.get("meta_cognition", 0.5),
                        random.uniform(-0.1, 0.1)
                    ]
                    simulated_gradients.append(np.array(grad_values))

            if simulated_gradients:
                # Calculate catastrophic forgetting resilience
                forgetting_resilience = self.formulas.catastrophic_forgetting_resilience(
                    simulated_gradients,
                    T=min(10, len(simulated_gradients)),
                    kappa=0.1
                )

                # Store gradients for future use
                self.gradient_history.extend(simulated_gradients)
                if len(self.gradient_history) > self.max_gradient_history:
                    self.gradient_history = self.gradient_history[-self.max_gradient_history:]

                # Update learning rate with adaptive plasticity
                if self.adaptive_plasticity and len(simulated_gradients) > 0:
                    # Use mean gradient as current parameters
                    current_params = np.mean(simulated_gradients, axis=0)
                    target_params = np.zeros_like(current_params)

                    new_lr = self.formulas.adaptive_plasticity(
                        theta=current_params,
                        theta_star=target_params,
                        eta0=self.learning_rate,
                        beta=0.1,
                        gamma=0.1,
                        uncertainty=1 - forgetting_resilience
                    )

                    # Bound learning rate
                    self.learning_rate = max(1e-6, min(0.1, new_lr))

                # Update learning efficiency metric
                self.metrics["learning_efficiency"] = forgetting_resilience

        except Exception as e:
            if self.config.get("debug", False):
                print(f"Learning error: {e}")

    def quantum_reasoning(self, graph: np.ndarray, query: str) -> Dict[str, Any]:
        """Perform quantum walk reasoning on knowledge graph"""
        try:
            if graph.size == 0:
                return {"error": "Empty graph", "confidence": 0.0}

            probabilities = self.formulas.quantum_walk_reasoning(
                graph_adjacency=graph,
                time_steps=min(10, graph.shape[0]),
                gamma=0.5
            )

            if probabilities.size == 0:
                return {"error": "No probabilities calculated", "confidence": 0.0}

            # Find most probable node
            most_probable = np.argmax(probabilities)
            confidence = float(probabilities[most_probable])

            return {
                "query": query,
                "probabilities": probabilities.tolist(),
                "most_probable_node": int(most_probable),
                "confidence": confidence,
                "success": True
            }

        except Exception as e:
            return {
                "query": query,
                "error": str(e),
                "confidence": 0.0,
                "success": False
            }

    def calculate_memory_capacity(self) -> Dict[str, float]:
        """Calculate holographic memory capacity"""
        try:
            # Estimate current memory usage
            episodic_mem = len(self.episodic_memory)
            semantic_mem = sum(len(v) for v in self.semantic_memory.values())

            total_memory = episodic_mem + semantic_mem

            # Calculate theoretical capacity (in cubic meters, simplified)
            # Assuming each memory entry takes ~1KB
            V = total_memory * 1024 / (8 * 10**9)  # Convert to approximate cubic meters

            theoretical_capacity = self.formulas.holographic_memory_capacity(V)

            utilization = (total_memory / theoretical_capacity * 100) if theoretical_capacity > 0 else 0

            return {
                "current_usage": total_memory,
                "theoretical_capacity": theoretical_capacity,
                "utilization_percentage": min(100.0, utilization),
                "episodic_memories": episodic_mem,
                "semantic_categories": len(self.semantic_memory),
                "success": True
            }

        except Exception as e:
            return {
                "error": str(e),
                "current_usage": 0,
                "success": False
            }

    def start_continuous_learning(self, interval: float = 0.5):
        """Start continuous learning thread"""
        if self.running:
            return

        self.running = True

        def learning_loop():
            while self.running:
                try:
                    self.learn_from_experience()
                    time.sleep(interval)
                except Exception as e:
                    if self.config.get("debug", False):
                        print(f"Continuous learning error: {e}")
                    time.sleep(interval)

        self.learning_thread = threading.Thread(target=learning_loop, daemon=True)
        self.learning_thread.start()

        if self.config.get("debug", False):
            print(f"üìö Continuous learning started (interval: {interval}s)")

    def stop_continuous_learning(self):
        """Stop continuous learning"""
        self.running = False
        if self.learning_thread:
            self.learning_thread.join(timeout=2.0)

        if self.config.get("debug", False):
            print("üìö Continuous learning stopped")

    def get_status(self) -> Dict[str, Any]:
        """Get current system status"""
        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0

        return {
            "cognitive_state": {k: round(v, 4) for k, v in self.cognitive_state.items()},
            "metrics": {k: round(v, 4) for k, v in self.metrics.items()},
            "memory_stats": {
                "episodic": len(self.episodic_memory),
                "semantic_categories": len(self.semantic_memory),
                "semantic_items": sum(len(v) for v in self.semantic_memory.values()),
                "working": len(self.working_memory)
            },
            "performance": {
                "success_rate": self.success_count / max(1, self.success_count + self.error_count),
                "avg_processing_time": avg_processing_time,
                "total_processed": self.success_count + self.error_count,
                "error_count": self.error_count
            },
            "learning_rate": self.learning_rate,
            "running": self.running,
            "quantum_mode": self.config["quantum_mode"]
        }

# ============================================================
# üî¨ PART 3: DEMONSTRATION AND INTEGRATION (FIXED)
# ============================================================

class AGIDemonstration:
    """Demonstrate the AGI system capabilities - FIXED VERSION"""

    @staticmethod
    def run_comprehensive_demo():
        """Run comprehensive demonstration of all AGI formulas and capabilities"""
        print("=" * 70)
        print("AGI-SENTIENCE-QUANTUM UNIFIED SYSTEM DEMONSTRATION v2.1")
        print("=" * 70)

        # Initialize AGI Core with debug mode
        agi = AGICore({
            "quantum_mode": True,
            "sentience_threshold": 0.7,
            "entropy_weight": 0.4,
            "learning_rate": 0.001,
            "memory_capacity": 100,
            "debug": True
        })

        print("\n1Ô∏è‚É£ Testing Dimensional Collapse Emergence:")
        result = agi.formulas.dimensional_collapse_emergence(
            d=5, n=100, lambdas=[0.1, 0.2, 0.3, 0.4, 0.5],
            betas=[0.5, 0.6, 0.7, 0.8, 0.9], C=[1, 2, 3, 4, 5]
        )
        print(f"   Emergence Level: {result:.4f}")

        print("\n2Ô∏è‚É£ Testing Cross-Modal Synthesis:")
        modalities = [
            np.random.randn(100),  # Vision
            np.random.randn(100),  # Audio
            np.random.randn(100)   # Language
        ]
        synthesis = agi.formulas.cross_modal_synthesis(modalities, sigma=1.0)
        print(f"   Synthesis Index: {synthesis:.4f}")

        print("\n3Ô∏è‚É£ Testing Recursive Attention:")
        seq_len = 5
        d_model = 4
        Q = np.random.randn(seq_len, d_model)
        K = np.random.randn(seq_len, d_model)
        V = np.random.randn(seq_len, d_model)

        attention_result = agi.formulas.recursive_attention(
            n=2, h=2, Q=Q, K=K, V=V, d_k=d_model
        )
        print(f"   Attention Output Shape: {attention_result.shape}")
        print(f"   Mean Attention Value: {np.mean(attention_result):.4f}")

        print("\n4Ô∏è‚É£ Testing Catastrophic Forgetting Resilience:")
        gradients = [np.random.randn(10) for _ in range(20)]
        resilience = agi.formulas.catastrophic_forgetting_resilience(
            gradients, T=10, kappa=0.1
        )
        print(f"   Resilience Score: {resilience:.4f}")

        print("\n5Ô∏è‚É£ Testing Adaptive Plasticity (FIXED):")
        theta = np.random.randn(10)
        theta_star = np.zeros(10)
        adaptive_lr = agi.formulas.adaptive_plasticity(
            theta, theta_star, eta0=0.01, beta=0.1, gamma=0.1, uncertainty=0.3
        )
        print(f"   Adaptive Learning Rate: {adaptive_lr:.6f}")  # Now a scalar!

        print("\n6Ô∏è‚É£ Testing Quantum Coherent Memory:")
        quantum_state = agi.formulas.quantum_coherent_memory(
            N=5, E_values=[1.0, 2.0, 3.0, 4.0, 5.0], gamma_values=[0.01]*5
        )
        print(f"   Quantum State: {quantum_state}")
        print(f"   Magnitude: {abs(quantum_state):.4f}")

        print("\n7Ô∏è‚É£ Testing Quantum Walk Reasoning:")
        graph = np.random.rand(5, 5)
        graph = (graph + graph.T) / 2  # Make symmetric
        np.fill_diagonal(graph, 0)

        walk_result = agi.formulas.quantum_walk_reasoning(graph, time_steps=5, gamma=0.5)
        print(f"   Node Probabilities: {[f'{p:.3f}' for p in walk_result]}")
        print(f"   Most Probable Node: {np.argmax(walk_result)}")

        print("\n8Ô∏è‚É£ Testing Holographic Memory Capacity:")
        capacity = agi.formulas.holographic_memory_capacity(V=0.001)  # 1 liter
        print(f"   Theoretical Capacity: {capacity:.2e} bits")

        print("\n" + "=" * 70)
        print("AGI CORE INTEGRATION TEST")
        print("=" * 70)

        # Test AGI Core processing
        print("\nüîß Processing different input types:")

        # Text input
        print("   Processing text input...")
        text_result = agi.process_input("Hello, AGI system. How are you today?")
        print(f"   Response: {text_result['response']}")
        print(f"   Success: {text_result['success']}")
        print(f"   Awareness: {text_result['cognitive_state']['awareness']:.4f}")

        # Numerical input
        print("\n   Processing numerical input...")
        numeric_result = agi.process_input(np.random.randn(20))
        print(f"   Response: {numeric_result['response']}")
        print(f"   Coherence: {numeric_result['cognitive_state']['coherence']:.4f}")

        # Multi-modal input
        print("\n   Processing multi-modal input...")
        multimodal_input = {
            "text": "A cat on a mat",
            "image_features": np.random.randn(128),
            "audio_features": np.random.randn(64)
        }
        multimodal_result = agi.process_input(multimodal_input)
        print(f"   Response: {multimodal_result['response']}")
        print(f"   Meta-cognition: {multimodal_result['cognitive_state']['meta_cognition']:.4f}")
        print(f"   AGI Score: {multimodal_result['metrics']['agi_score']:.4f}")

        # Start continuous learning
        print("\nüöÄ Starting continuous learning...")
        agi.start_continuous_learning(interval=0.3)

        # Simulate some learning cycles
        print("   Simulating learning cycles...")
        for i in range(5):
            # Process more inputs
            result = agi.process_input(f"Learning sample {i+1}")

            # Check status
            status = agi.get_status()
            print(f"   Cycle {i+1}: "
                  f"AGI Score = {status['metrics']['agi_score']:.4f}, "
                  f"Sentience = {status['metrics']['sentience_index']:.4f}, "
                  f"Success Rate = {status['performance']['success_rate']:.2f}")

            time.sleep(0.2)

        # Stop learning
        agi.stop_continuous_learning()

        # Test quantum reasoning
        print("\nüåå Testing Quantum Reasoning on Knowledge Graph:")
        knowledge_graph = np.array([
            [0, 1, 0, 0, 1],
            [1, 0, 1, 0, 0],
            [0, 1, 0, 1, 0],
            [0, 0, 1, 0, 1],
            [1, 0, 0, 1, 0]
        ], dtype=float)
        reasoning_result = agi.quantum_reasoning(knowledge_graph, "Find related concepts")
        print(f"   Success: {reasoning_result.get('success', False)}")
        if reasoning_result.get('success'):
            print(f"   Most probable node: {reasoning_result['most_probable_node']}")
            print(f"   Confidence: {reasoning_result['confidence']:.4f}")

        # Check memory capacity
        print("\nüíæ Checking Memory Capacity:")
        memory_stats = agi.calculate_memory_capacity()
        if memory_stats.get('success'):
            print(f"   Current usage: {memory_stats['current_usage']} items")
            print(f"   Theoretical capacity: {memory_stats['theoretical_capacity']:.2e} bits")
            print(f"   Utilization: {memory_stats['utilization_percentage']:.6f}%")
        else:
            print(f"   Error: {memory_stats.get('error', 'Unknown error')}")

        # Final status
        print("\nüìä FINAL SYSTEM STATUS:")
        final_status = agi.get_status()

        print("   Cognitive State:")
        for key, value in final_status['cognitive_state'].items():
            print(f"     {key}: {value:.4f}")

        print("\n   Metrics:")
        for key, value in final_status['metrics'].items():
            print(f"     {key}: {value:.4f}")

        print("\n   Memory Stats:")
        for key, value in final_status['memory_stats'].items():
            print(f"     {key}: {value}")

        print("\n   Performance:")
        for key, value in final_status['performance'].items():
            if isinstance(value, float):
                print(f"     {key}: {value:.4f}")
            else:
                print(f"     {key}: {value}")

        print(f"\n   Learning Rate: {final_status['learning_rate']:.6f}")
        print(f"   Quantum Mode: {final_status['quantum_mode']}")
        print(f"   Running: {final_status['running']}")

        print("\n" + "=" * 70)
        print("DEMONSTRATION COMPLETE - AGI SYSTEM OPERATIONAL")
        print("=" * 70)

        return agi

# ============================================================
# üöÄ PART 4: MAIN ENTRY POINT (FIXED)
# ============================================================

def main():
    """Main entry point for the unified AGI system"""

    print("\n" + "=" * 70)
    print("AGI-SENTIENCE-QUANTUM UNIFIED SYSTEM v2.1")
    print("December 2025 - FIXED VERSION")
    print("=" * 70)

    # Check for command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "demo":
            AGIDemonstration.run_comprehensive_demo()
        elif sys.argv[1] == "interactive":
            run_interactive_mode()
        elif sys.argv[1] == "test":
            run_unit_tests()
        elif sys.argv[1] == "simple":
            run_simple_test()
        else:
            print(f"Unknown command: {sys.argv[1]}")
            print_usage()
    else:
        # Default: run comprehensive demo
        AGIDemonstration.run_comprehensive_demo()

def run_interactive_mode():
    """Run AGI system in interactive mode"""
    print("\nüîÆ AGI INTERACTIVE MODE")
    print("Type 'quit' to exit, 'status' for system status, 'help' for commands")

    agi = AGICore({"debug": True})
    agi.start_continuous_learning(interval=0.5)

    try:
        while True:
            user_input = input("\nYou: ").strip()

            if user_input.lower() == 'quit':
                break
            elif user_input.lower() == 'status':
                status = agi.get_status()
                print("\nSystem Status:")
                print("  Cognitive State:")
                for key, value in status['cognitive_state'].items():
                    print(f"    {key}: {value:.4f}")
                print("  Metrics:")
                for key, value in status['metrics'].items():
                    print(f"    {key}: {value:.4f}")
                continue
            elif user_input.lower() == 'help':
                print("\nCommands:")
                print("  quit - Exit the program")
                print("  status - Show system status")
                print("  clear - Clear memory")
                print("  learn - Force learning cycle")
                print("  graph - Test quantum reasoning")
                print("  memory - Show memory stats")
                continue
            elif user_input.lower() == 'clear':
                agi.episodic_memory.clear()
                agi.semantic_memory.clear()
                agi.working_memory.clear()
                print("Memory cleared")
                continue
            elif user_input.lower() == 'learn':
                agi.learn_from_experience()
                print("Learning cycle completed")
                continue
            elif user_input.lower() == 'graph':
                # Create a simple graph
                graph = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=float)
                result = agi.quantum_reasoning(graph, "Test query")
                print(f"Quantum reasoning result: {result}")
                continue
            elif user_input.lower() == 'memory':
                stats = agi.calculate_memory_capacity()
                print(f"Memory stats: {stats}")
                continue

            # Process user input
            result = agi.process_input(user_input)

            if result['success']:
                response = result['response']
                if isinstance(response, dict) and 'action' in response:
                    print(f"\nAGI: {response['action']} (confidence: {response.get('confidence', 0.0):.2f})")
                else:
                    print(f"\nAGI: {response}")

                # Show brief status
                print(f"[Awareness: {result['cognitive_state']['awareness']:.3f}, "
                      f"Sentience: {result['metrics']['sentience_index']:.3f}, "
                      f"Time: {result['processing_time']:.3f}s]")
            else:
                print(f"\nAGI Error: {result.get('error', 'Unknown error')}")

    except KeyboardInterrupt:
        print("\n\nInterrupted by user")
    except Exception as e:
        print(f"\nError: {e}")
    finally:
        agi.stop_continuous_learning()
        print("\nAGI system shut down")

def run_unit_tests():
    """Run unit tests for AGI formulas"""
    print("\nüß™ RUNNING UNIT TESTS")

    formulas = AGIFormulas()
    tests_passed = 0
    total_tests = 0

    # Test 1: Dimensional Collapse
    try:
        result = formulas.dimensional_collapse_emergence(
            d=3, n=10, lambdas=[0.1, 0.2, 0.3], betas=[0.5, 0.5, 0.5], C=[1, 1, 1]
        )
        assert 0 <= result <= 1, "Result out of bounds"
        tests_passed += 1
        print("‚úì Dimensional Collapse: PASSED")
    except Exception as e:
        print(f"‚úó Dimensional Collapse: FAILED - {e}")
    total_tests += 1

    # Test 2: Cross-Modal Synthesis
    try:
        modalities = [np.random.randn(10), np.random.randn(10)]
        result = formulas.cross_modal_synthesis(modalities)
        assert isinstance(result, float), "Result not float"
        assert 0 <= result <= 2, "Result out of expected range"
        tests_passed += 1
        print("‚úì Cross-Modal Synthesis: PASSED")
    except Exception as e:
        print(f"‚úó Cross-Modal Synthesis: FAILED - {e}")
    total_tests += 1

    # Test 3: Catastrophic Forgetting
    try:
        gradients = [np.random.randn(5) for _ in range(5)]
        result = formulas.catastrophic_forgetting_resilience(gradients, T=5, kappa=0.1)
        assert 0 <= result <= 1, "Result out of bounds"
        tests_passed += 1
        print("‚úì Catastrophic Forgetting: PASSED")
    except Exception as e:
        print(f"‚úó Catastrophic Forgetting: FAILED - {e}")
    total_tests += 1

    # Test 4: Adaptive Plasticity (FIXED)
    try:
        theta = np.random.randn(10)
        theta_star = np.zeros(10)
        result = formulas.adaptive_plasticity(theta, theta_star, eta0=0.01, beta=0.1, gamma=0.1, uncertainty=0.3)
        assert isinstance(result, float), "Result not float"
        assert result > 0, "Learning rate should be positive"
        tests_passed += 1
        print("‚úì Adaptive Plasticity: PASSED")
    except Exception as e:
        print(f"‚úó Adaptive Plasticity: FAILED - {e}")
    total_tests += 1

    # Test 5: Quantum Memory
    try:
        result = formulas.quantum_coherent_memory(3, [1, 2, 3], [0.01, 0.01, 0.01])
        assert isinstance(result, complex), "Result not complex"
        tests_passed += 1
        print("‚úì Quantum Memory: PASSED")
    except Exception as e:
        print(f"‚úó Quantum Memory: FAILED - {e}")
    total_tests += 1

    # Test 6: Quantum Walk
    try:
        graph = np.array([[0, 1], [1, 0]], dtype=float)
        result = formulas.quantum_walk_reasoning(graph, time_steps=2, gamma=0.5)
        assert isinstance(result, np.ndarray), "Result not array"
        assert np.all(result >= 0), "Probabilities should be non-negative"
        assert abs(np.sum(result) - 1.0) < 0.01, "Probabilities should sum to 1"
        tests_passed += 1
        print("‚úì Quantum Walk: PASSED")
    except Exception as e:
        print(f"‚úó Quantum Walk: FAILED - {e}")
    total_tests += 1

    print(f"\nüìä Test Results: {tests_passed}/{total_tests} passed")

    if tests_passed == total_tests:
        print("‚úÖ All tests passed!")
    else:
        print("‚ö†Ô∏è Some tests failed")

def run_simple_test():
    """Run a simple test of the AGI system"""
    print("\nüß† SIMPLE AGI TEST")

    # Create a simple AGI instance
    agi = AGICore({
        "quantum_mode": False,
        "memory_capacity": 50,
        "debug": False
    })

    # Test some basic interactions
    test_inputs = [
        "Hello, I'm testing the AGI system.",
        np.array([0.1, 0.5, 0.9, 0.3, 0.7]),
        {"text": "Test message", "value": 42}
    ]

    for i, input_data in enumerate(test_inputs):
        print(f"\nTest {i+1}: {type(input_data).__name__}")
        result = agi.process_input(input_data)

        if result['success']:
            response = result['response']
            print(f"  Response: {response.get('action', 'unknown')}")
            print(f"  Confidence: {response.get('confidence', 0.0):.2f}")
            print(f"  Processing time: {result['processing_time']:.3f}s")
        else:
            print(f"  Error: {result.get('error', 'Unknown')}")

    # Show final status
    status = agi.get_status()
    print(f"\nFinal AGI Score: {status['metrics']['agi_score']:.3f}")
    print(f"Final Sentience Index: {status['metrics']['sentience_index']:.3f}")
    print(f"Total processed: {status['performance']['total_processed']}")

def print_usage():
    """Print usage instructions"""
    print("\nUsage: python unified_agi.py [command]")
    print("\nCommands:")
    print("  demo        - Run comprehensive demonstration")
    print("  interactive - Run in interactive mode")
    print("  test        - Run unit tests")
    print("  simple      - Run simple test")
    print("\nExample: python unified_agi.py interactive")

# ============================================================
# üìä RUN THE SYSTEM
# ============================================================

if __name__ == "__main__":
    main()
