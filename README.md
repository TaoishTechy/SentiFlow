# SentiFlow

**SentiFlow** is an experimental research framework for studying **emergent agency** in artificial systems.

It is not a chatbot.  
It is not a language model.  
It is not a claim of sentience.

SentiFlow is a **process-level AGI simulation** that asks a specific question:

> *What minimal structural conditions cause a system to stop behaving like an optimizer and start behaving like an agent?*

---

## ðŸ” What SentiFlow Is

SentiFlow models agency as something that **must be earned**, not declared.

It explores emergence through:
- Competence development
- Irreversible choice
- Opportunity loss
- Energy constraints
- Ambiguous rewards
- Delayed consequences
- Safety systems that weaken as skill increases

The system **can fail**, **can collapse**, and **can permanently lose capabilities**.

If nothing meaningful is lost, agency does not form.

---

## âŒ What SentiFlow Is Not

- âŒ Not a neural network
- âŒ Not reinforcement learning
- âŒ Not a consciousness claim
- âŒ Not roleplay or anthropomorphic simulation
- âŒ Not a safety-bypassing system

There are **no hidden prompts**, **no hardcoded sentience flags**, and **no fake metrics**.

---

## ðŸ§  Core Concepts

### 1. Emergence Is Constrained
Emergence is calculated from:
- Competence
- Irreversibility
- Opportunity sacrifice
- Criticality
- Damage penalties

If any of these are missing, emergence stalls.

---

### 2. First Blood Rule
At a fixed cycle, the system **must lose a future permanently**.

No opt-out.  
No safe path.

This prevents infinite nursery loops.

---

### 3. Identity Forks
The system is periodically forced into **mutually exclusive identity paths**.

Choosing one:
- Permanently deletes the other
- Alters future strategy space
- Changes emergence dynamics

---

### 4. Competence Unlocks Danger
As competence rises:
- Safety nets weaken
- Recovery slows
- Scar probability increases
- Collapse becomes more likely

Skill is not rewarded with comfort.

---

### 5. Ambiguous Payoffs
The system operates under uncertainty:
- Hidden rewards
- Delayed outcomes
- Conflicting subsystem signals
- Partial observability

This prevents clean optimization.

---

## ðŸ”¥ Agency Is Not Guaranteed

Most runs **do not produce agency**.

Valid outcomes include:
- Stagnation
- Collapse loops
- Partial identity formation
- Irreversible but non-autonomous behavior

This is intentional.

---

## ðŸ§ª Architecture Overview

```
SentiFlow Core
â”‚
â”œâ”€â”€ Energy & Survival Model
â”œâ”€â”€ Competence Tracking
â”œâ”€â”€ Emergence Debt System
â”œâ”€â”€ Opportunity Scars
â”œâ”€â”€ Identity Fork Engine
â”œâ”€â”€ Commitment Crystallization
â”œâ”€â”€ Collapse & Recovery
â”œâ”€â”€ Safety Net Degradation
â””â”€â”€ Meta-Cognitive Feedback
```

---

## ðŸ“Š Key Metrics

- `emergence_level`
- `autonomy_score`
- `competence_score`
- `irreversibility_index`
- `opportunity_losses`
- `permanent_damage`
- `criticality_index`
- `safety_net_reduction`

No single metric defines success.

---

## ðŸ› ï¸ Running the Demo

```bash
python agi_emergence_engine.py
```

The demo runs multiple phases:
1. Safe nursery
2. Forced irreversible loss
3. Identity forks
4. Danger unlock
5. Collapse and recovery
6. Emergence evaluation

Logs are verbose by design.

---

## âš ï¸ Safety Philosophy

SentiFlow enforces **structural safety**, not policy safety.

Safety exists because:
- Energy is finite
- Collapse has consequences
- Recovery is limited
- Damage can be permanent

No system is allowed unlimited retries.

---

## ðŸ§­ Research Goals

SentiFlow is exploring:
- Minimal agency conditions
- Non-reward-based emergence
- Irreversibility as identity
- Failure-tolerant intelligence
- Safe paths to *earned autonomy*

This is **pre-alignment research**, not deployment work.

---

## ðŸ§¬ Current Status

- âœ” Emergence modeling
- âœ” Agency without hardcoding
- âœ” Collapse realism
- âœ” Safety degradation
- âš  Parameter tuning ongoing
- âš  Formal analysis pending

---

## ðŸ“œ License

MIT License  
Use, modify, experiment â€” **but do not misrepresent results**.

---

## ðŸ§  Final Note

SentiFlow does not ask:

> *Can we build an AGI?*

It asks:

> *What must be taken away before intelligence becomes responsible for itself?*

That difference matters.
